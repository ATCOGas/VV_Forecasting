{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset for Faults Analysis against Weather Data\n",
    "Munge datasets from multiple sources into a single dataframe for analysis\n",
    "<br>\n",
    "Step 1: [Setup file and folder paths](#setup)<br>\n",
    "Step 2: [Build dataframe from multiple files](#multiple)<br>\n",
    "  or  : [Load previously build dataframe](#single)<br>\n",
    "Step 3: [Clean dataframe](#clean)<br>\n",
    "Step 4: [Combine into single dataframe](#combine)<br>\n",
    "Step 5: [Write dataframe to file](#write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup file and folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working folder: C:\\Users\\hsmith\\Desktop\\Core Skills\\VV_Forecasting\\notebooks\n",
      "Current data folder: C:\\Users\\hsmith\\Desktop\\Core Skills\\VV_Forecasting\\data\n",
      "Current output folder: C:\\Users\\hsmith\\Desktop\\Core Skills\\VV_Forecasting\\output\n",
      "BOM data folder: C:\\Users\\hsmith\\Desktop\\Core Skills\\VV_Forecasting\\data\\BOM\n",
      "Files in data folder:['BOM', 'BOM Station Locations.xlsx', 'Book1.xlsx', 'corrective_maint_against_weather.csv', 'corrective_maint_job_types.csv', 'corrective_maint_job_types.xlsx', 'DS FLOC.xlsx', 'FLOC (minus DS).xlsx', 'HSclean_postcodes_stn.csv', 'HSdf_complete.csv', 'HSdf_maint(afterpostcode).csv', 'HSdf_maint(b4postcode).csv', 'Notifs 2009-2018.xlsx', 'WA_Postcodes.xlsx', 'Weather.xlsx', 'weather_predictions.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "data_folder = path.join(path.abspath('..'), 'data')\n",
    "bom_data_folder = path.join(path.abspath('..'), 'data\\BOM')\n",
    "output_folder = path.join(path.abspath('..'), 'output')\n",
    "\n",
    "print ('Current working folder: ' + current_folder)\n",
    "print ('Current data folder: ' + data_folder)\n",
    "print ('Current output folder: ' + output_folder)\n",
    "print ('BOM data folder: ' + bom_data_folder)\n",
    "print ('Files in data folder:' + str(os.listdir(data_folder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframe from previously built xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'corrective_maint_against_weather.csv'\n",
    "#filename = 'binary_weather.csv'\n",
    "#df_complete = pd.read_csv(path.join(data_folder, filename))\n",
    "#print ('File loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiple'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build single weather dataframe from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import multiple CSV files from BOM into one dataframe\n",
    "# Takes about a minute to run...\n",
    "import glob\n",
    "\n",
    "all_stations = pd.DataFrame()\n",
    "\n",
    "folder_list = os.listdir(bom_data_folder)\n",
    "for folder in folder_list:\n",
    "    csv_list = glob.glob(path.join(path.abspath(bom_data_folder), folder+'\\*.csv'))\n",
    "    df = pd.concat([pd.read_csv(f, header=None, skiprows=13) for f in csv_list], ignore_index=True)\n",
    "    all_stations = all_stations.append(df , ignore_index=False)\n",
    "\n",
    "# Add dataframe column headings\n",
    "all_stations.columns=['station_name', 'weather_date', 'evapo_trans_0000_2400', 'rain_0900_0900', 'pan_evap_0900_0900',\n",
    "                          'max_temp','min_temp', 'max_rel_humidity','min_rel_humidity', 'avg_10m_wind_speed','solar_radiation']\n",
    "\n",
    "# Remove totals rows and reindex\n",
    "all_stations = all_stations[all_stations['station_name'] != 'Totals:']\n",
    "all_stations = all_stations.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data into individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maintenance data loaded.\n",
      "Domestic service assets loaded.\n",
      "Other assets loaded.\n",
      "Post codes loaded.\n",
      "Weather stations loaded.\n",
      "Weather data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data into individual dataframes\n",
    "# Takes about two minutes to load...\n",
    "\n",
    "df_notifs = pd.ExcelFile(path.join(data_folder, 'Notifs 2009-2018.xlsx')).parse('Sheet1')\n",
    "print ('Maintenance data loaded.')\n",
    "df_ds_floc_master_data = pd.ExcelFile(path.join(data_folder, 'DS FLOC.xlsx')).parse('DS FLOC')\n",
    "print ('Domestic service assets loaded.')\n",
    "df_else_floc_master_data = pd.ExcelFile(path.join(data_folder, 'FLOC (minus DS).xlsx')).parse('FL')\n",
    "print ('Other assets loaded.')\n",
    "df_postcodes = pd.ExcelFile(path.join(data_folder, 'WA_Postcodes.xlsx')).parse('postcodes')\n",
    "print ('Post codes loaded.')\n",
    "df_stations = pd.ExcelFile(path.join(data_folder, 'BOM Station Locations.xlsx')).parse('Stations')\n",
    "print ('Weather stations loaded.')\n",
    "df_weather = pd.ExcelFile(path.join(data_folder, 'Weather.xlsx')).parse('Sheet1')\n",
    "print ('Weather data loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data has loaded as intended\n",
    "\n",
    "#df_notifs.head()\n",
    "#df_ds_floc_master_data.head()\n",
    "#df_else_floc_master_data.head()\n",
    "#df_postcodes.head()\n",
    "#df_stations.head()\n",
    "#df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data loaded into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip out all columns that will not be explicitly used by this particular analysis. These 'cleaning' algorithms will need to be tweaked for different analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_notifs dataframe\n",
    "There are several code blocks below that build a dataframe for notifications analysis. Depending on the anlysis you want to do, select the appropriate code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_notifs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Failure Analysis Dataframe\n",
    "Use this code block to form dataframe for failure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_notifs\n",
    "\n",
    "df.set_index('Notification')\n",
    "clean = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Fields that can be directly copied to clean dataframe\n",
    "make_copy = (\n",
    "    ('Notification', 'notif'),\n",
    "    ('Notif.date', 'notif_date')\n",
    "    \n",
    ")\n",
    "for orig, new in make_copy:\n",
    "    clean[new] = df[orig]\n",
    "\n",
    "# Numeric columns\n",
    "clean['notif'] = pd.to_numeric(df['Notification']*1., errors='coerce')# recast notification number from int64 to float64\n",
    "clean['floc'] = pd.to_numeric(df['Functional Loc.'], errors='coerce')\n",
    "clean['order'] = pd.to_numeric(df['Order'], errors='coerce')\n",
    "\n",
    "# Categorical columns\n",
    "# TODO: Maintain spreadsheet containing lists of all these codes and load them for application here\n",
    "make_categorical = (  # has column, new_name, category pairs\n",
    "    ('Job Type', 'job_type', None),\n",
    "    ('ObjectPartCode', 'object_part_code', None),\n",
    "    ('ObjPartCodeText', 'object_part_text', None),\n",
    "    ('Damage Code', 'damage_code', None),\n",
    "    ('Prob. code text', 'damage_code_text', None),\n",
    "    ('Cause code', 'cause_code', None),\n",
    "    ('Cause code text', 'cause_code_text', None),\n",
    "    ('Unit', 'duration_units', None),\n",
    "    ('Breakdown', 'breakdown', ('X','')),\n",
    "    ('Object Code group', 'object_code_group', None),\n",
    "    ('Obj.p. grp.txt.', 'object_group_text', None),\n",
    "    ('Notifictn type', 'notif_type', ('SP','SF')),\n",
    "    ('Cause Code group', 'cause_code_group', None),\n",
    "    ('Cause grp. text', 'cause_group_text', None)\n",
    ")\n",
    "\n",
    "for column, new_name, cats in make_categorical:\n",
    "    clean[new_name] = pd.Categorical(df[column], categories=cats)\n",
    "\n",
    "clean_notifs = clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Variable Volume Job Type Analysis Dataframe\n",
    "Use this code block to form dataframe for VV Forecasting analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_notifs\n",
    "\n",
    "df.set_index('Notification')\n",
    "clean = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Fields that can be directly copied to clean dataframe\n",
    "make_copy = (\n",
    "    ('Notification', 'notif'),\n",
    "    ('Notif.date', 'notif_date')\n",
    "    \n",
    ")\n",
    "for orig, new in make_copy:\n",
    "    clean[new] = df[orig]\n",
    "\n",
    "# Numeric columns\n",
    "clean['notif'] = pd.to_numeric(df['Notification']*1., errors='coerce')# recast notification number from int64 to float64\n",
    "clean['floc'] = pd.to_numeric(df['Functional Loc.'], errors='coerce')\n",
    "clean['order'] = pd.to_numeric(df['Order'], errors='coerce')\n",
    "\n",
    "# Categorical columns\n",
    "clean['job_type'] = pd.Categorical(df['Job Type'], categories=None)\n",
    "\n",
    "clean_notifs = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_notifs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_ds_floc_master_data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains master data associated with all domestic gas distribution services in WA\n",
    "Primary key is TPLNR field,(functional location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ds_floc_master_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ds_floc_master_data\n",
    "\n",
    "df.set_index('TPLNR')\n",
    "clean = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Fields that can be directly copied to clean dataframe\n",
    "make_copy = (\n",
    "    ('MTRMD_I', 'meter_model'),\n",
    "    ('MTRDT_I', 'meter_install_date')\n",
    ")\n",
    "for orig, new in make_copy:\n",
    "    clean[new] = df[orig]\n",
    "\n",
    "# Numeric columns\n",
    "clean['floc'] = pd.to_numeric(df['TPLNR'], errors='coerce')\n",
    "clean['supply_pressure'] = pd.to_numeric(df['SUP_PRS'], errors='coerce')\n",
    "clean['postcode'] = pd.to_numeric(df['PCODE'], errors='coerce')\n",
    "\n",
    "# Categorical columns\n",
    "make_categorical = (  # has column, new_name, category pairs\n",
    "    ('RBNR', 'catalog_profile', None),\n",
    "    ('LOCN', 'network', ('NM','SM','MA','BU','BS','KA','AL','GE'))\n",
    ")\n",
    "\n",
    "for column, new_name, cats in make_categorical:\n",
    "    clean[new_name] = pd.Categorical(df[column], categories=cats)\n",
    "\n",
    "clean_ds_floc_master_data = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_ds_floc_master_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_else_floc_master_data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains master data associated with all ATCO Gas assets other than domestic services loaded above\n",
    "Primary key:TPLNR    #functional location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_else_floc_master_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_else_floc_master_data\n",
    "\n",
    "df.set_index('Functional Loc.')\n",
    "clean = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Numeric columns\n",
    "clean['floc'] = pd.to_numeric(df['Functional Loc.'], errors='coerce')\n",
    "clean['postcode'] = pd.to_numeric(df['Postal Code'], errors='coerce')\n",
    "\n",
    "# Categorical columns\n",
    "make_categorical = (  # has column, new_name, category pairs\n",
    "    ('Catalog profile', 'catalog_profile', ('MAIN','GATESTN','REGSET','METERSET','VALVE','RECTFR','SERV_LINE','COMMETER')),\n",
    "    ('Location', 'network', ('NM','SM','MA','BU','BS','KA','AL','GE')),\n",
    "    ('City', 'suburb', None)\n",
    ")\n",
    "\n",
    "for column, new_name, cats in make_categorical:\n",
    "    clean[new_name] = pd.Categorical(df[column], categories=cats)\n",
    "\n",
    "clean_else_floc_master_data = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_else_floc_master_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_postcodes dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains WA postcodes and their associated lat/lon.\n",
    "Primary key: postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_postcodes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_postcodes\n",
    "\n",
    "df.set_index('postcode')\n",
    "clean = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Numeric columns\n",
    "clean['postcode'] = pd.to_numeric(df['postcode'], errors='coerce')\n",
    "clean['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "clean['lon'] = pd.to_numeric(df['long'], errors='coerce')\n",
    "\n",
    "# Categorical column\n",
    "clean['suburb'] = pd.Categorical(df['locality'], categories=None)\n",
    "\n",
    "#clean = clean.set_index('postcode')\n",
    "clean_postcodes = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_postcodes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_stations dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains a list of BOM weather stations in WA and their associated lat/lon.\n",
    "Primary key: STN ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_stations\n",
    "\n",
    "df.set_index('STN ID')\n",
    "clean = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Field that can be directly copied to clean dataframe\n",
    "clean['station_id'] = df['STN ID']\n",
    "\n",
    "# Numeric columns\n",
    "clean['lat'] = pd.to_numeric(df['LAT'], errors='coerce')\n",
    "clean['lon'] = pd.to_numeric(df['LON'], errors='coerce')\n",
    "\n",
    "# Categorical columns\n",
    "clean['station_name'] = pd.Categorical(df['NAME'], categories=None)\n",
    "\n",
    "clean = clean.set_index('station_id')\n",
    "clean_stations = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_stations.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_weather dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains a list of features obtained from BOM weather stations in WA.\n",
    "Primary key: None. >> Link with Station name and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_weather\n",
    "df = all_stations\n",
    "clean = pd.DataFrame()\n",
    "\n",
    "# Datetime columns\n",
    "clean['weather_date'] = pd.to_datetime(df['weather_date'],format='%d/%m/%Y')\n",
    "\n",
    "# Numeric columns\n",
    "clean['evapo_trans_0000_2400'] = pd.to_numeric(df['evapo_trans_0000_2400'], errors='coerce')\n",
    "clean['rain_0900_0900'] = pd.to_numeric(df['rain_0900_0900'], errors='coerce')\n",
    "clean['pan_evap_0900_0900'] = pd.to_numeric(df['pan_evap_0900_0900'], errors='coerce')\n",
    "clean['max_temp'] = pd.to_numeric(df['max_temp'], errors='coerce')\n",
    "clean['min_temp'] = pd.to_numeric(df['min_temp'], errors='coerce')\n",
    "clean['max_rel_humidity'] = pd.to_numeric(df['max_rel_humidity'], errors='coerce')\n",
    "clean['min_rel_humidity'] = pd.to_numeric(df['min_rel_humidity'], errors='coerce')\n",
    "clean['avg_10m_wind_speed'] = pd.to_numeric(df['avg_10m_wind_speed'], errors='coerce')\n",
    "clean['solar_radiation'] = pd.to_numeric(df['solar_radiation'], errors='coerce')\n",
    "\n",
    "# Categorical columns\n",
    "clean['station_name'] = pd.Categorical(df['station_name'], categories=None)\n",
    "\n",
    "#clean = clean.set_index('station_name')\n",
    "clean = clean.set_index('weather_date')\n",
    "\n",
    "# Fill missing values\n",
    "clean = clean.fillna(method='ffill')\n",
    "\n",
    "clean_weather = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save concatenated files into a single file so above code does not need to continually be rerun\n",
    "# Takes about one minute to run\n",
    "\n",
    "output_filename = 'weather.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "output = pd.ExcelWriter(path.join(data_folder, output_filename), engine='xlsxwriter')\n",
    "clean_weather.to_excel(output, sheet_name)\n",
    "output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframes into a single dataframe to be used for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine asset master data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join DS functional locations and other asset functional locations into a single dataframe\n",
    "df = clean_ds_floc_master_data.append(clean_else_floc_master_data, sort=True)\n",
    "\n",
    "df_assets = df.drop_duplicates(subset=['floc'])\n",
    "df_assets = df_assets.set_index('floc')\n",
    "\n",
    "#df_assets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add asset data to maintenance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notif</th>\n",
       "      <th>notif_date</th>\n",
       "      <th>floc</th>\n",
       "      <th>order</th>\n",
       "      <th>job_type</th>\n",
       "      <th>catalog_profile</th>\n",
       "      <th>meter_install_date</th>\n",
       "      <th>meter_model</th>\n",
       "      <th>network</th>\n",
       "      <th>postcode</th>\n",
       "      <th>suburb</th>\n",
       "      <th>supply_pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300713965.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>504107.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SM</td>\n",
       "      <td>6110.0</td>\n",
       "      <td>SOUTHERN RIVER</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300719006.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>673096.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPL</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SM</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>KELMSCOTT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300719010.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>810841.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NM</td>\n",
       "      <td>6019.0</td>\n",
       "      <td>WEMBLEY DOWNS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300719430.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NM</td>\n",
       "      <td>6026.0</td>\n",
       "      <td>WOODVALE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300730505.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>117547.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SF1</td>\n",
       "      <td>DOMMETER</td>\n",
       "      <td>14.01.2014</td>\n",
       "      <td>M8A</td>\n",
       "      <td>SM</td>\n",
       "      <td>6153.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         notif notif_date      floc  order job_type catalog_profile  \\\n",
       "0  300713965.0 2011-12-21  504107.0    NaN      SPH            MAIN   \n",
       "1  300719006.0 2011-12-21  673096.0    NaN      SPL            MAIN   \n",
       "2  300719010.0 2011-12-21  810841.0    NaN      SPH            MAIN   \n",
       "3  300719430.0 2011-12-21      27.0    NaN      SPH            MAIN   \n",
       "4  300730505.0 2011-12-21  117547.0    NaN      SF1        DOMMETER   \n",
       "\n",
       "  meter_install_date meter_model network  postcode          suburb  \\\n",
       "0                NaN         NaN      SM    6110.0  SOUTHERN RIVER   \n",
       "1                NaN         NaN      SM    6111.0       KELMSCOTT   \n",
       "2                NaN         NaN      NM    6019.0   WEMBLEY DOWNS   \n",
       "3                NaN         NaN      NM    6026.0        WOODVALE   \n",
       "4         14.01.2014         M8A      SM    6153.0             NaN   \n",
       "\n",
       "   supply_pressure  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4             1.25  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join combined asset master data above to maintenance dataframe\n",
    "df_maint = clean_notifs.merge(df_assets, left_on='floc', right_on='floc', how='left')\n",
    "\n",
    "#df_maint = df_maint.set_index('notif')\n",
    "\n",
    "df_maint.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Station dataframe with postcodes dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances from each suburb centroid to idently closest weather station and then add station row to suburb dataframe\n",
    "# Takes about three minutes to run\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "result = pd.DataFrame()\n",
    "clean_postcodes_stn = pd.DataFrame()\n",
    "\n",
    "for postcode, pc_row in clean_postcodes.iterrows():\n",
    "    min_dist = 1000\n",
    "    for station_id, stn_row in clean_stations.iterrows():\n",
    "        # Use Pythagoras, not accurate but I don't know how to make it better...\n",
    "        dist = ((pc_row['lat']-stn_row['lat'])**2 + (pc_row['lon']-stn_row['lon'])**2)**0.5 \n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            p_code = pc_row['postcode']\n",
    "            stn_name = stn_row['station_name']\n",
    "        \n",
    "        result = pd.DataFrame({'postcode': [p_code],\n",
    "                               'station_name': [stn_name],\n",
    "                               'distance': [min_dist]})\n",
    "        \n",
    "    result = result.reset_index(drop=True) \n",
    "    clean_postcodes_stn = clean_postcodes_stn.append(result, ignore_index=True)\n",
    "\n",
    "clean_postcodes_stn.columns=['postcode', 'station_name', 'distance']   \n",
    "clean_postcodes_stn = clean_postcodes_stn.drop_duplicates(subset=['postcode'])\n",
    "\n",
    "#clean_postcodes_stn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_postcodes_stn\n",
    "csv_file = path.join(data_folder, 'HSclean_postcodes_stn.csv')\n",
    "clean_postcodes_stn.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = path.join(data_folder, 'HSdf_maint(b4postcode).csv')\n",
    "df_maint.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weather station name to maintenance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notif</th>\n",
       "      <th>notif_date</th>\n",
       "      <th>floc</th>\n",
       "      <th>order</th>\n",
       "      <th>job_type</th>\n",
       "      <th>catalog_profile</th>\n",
       "      <th>meter_install_date</th>\n",
       "      <th>meter_model</th>\n",
       "      <th>network</th>\n",
       "      <th>postcode</th>\n",
       "      <th>suburb</th>\n",
       "      <th>supply_pressure</th>\n",
       "      <th>station_name</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300713965.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>504107.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SM</td>\n",
       "      <td>6110.0</td>\n",
       "      <td>SOUTHERN RIVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JANDAKOT AERO</td>\n",
       "      <td>0.131159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300719006.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>673096.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPL</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SM</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>KELMSCOTT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BICKLEY</td>\n",
       "      <td>0.099496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300719010.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>810841.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NM</td>\n",
       "      <td>6019.0</td>\n",
       "      <td>WEMBLEY DOWNS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SWANBOURNE</td>\n",
       "      <td>0.049014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300719430.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NM</td>\n",
       "      <td>6026.0</td>\n",
       "      <td>WOODVALE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERTH METRO</td>\n",
       "      <td>0.138307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300730505.0</td>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>117547.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SF1</td>\n",
       "      <td>DOMMETER</td>\n",
       "      <td>14.01.2014</td>\n",
       "      <td>M8A</td>\n",
       "      <td>SM</td>\n",
       "      <td>6153.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>JANDAKOT AERO</td>\n",
       "      <td>0.087419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         notif notif_date      floc  order job_type catalog_profile  \\\n",
       "0  300713965.0 2011-12-21  504107.0    NaN      SPH            MAIN   \n",
       "1  300719006.0 2011-12-21  673096.0    NaN      SPL            MAIN   \n",
       "2  300719010.0 2011-12-21  810841.0    NaN      SPH            MAIN   \n",
       "3  300719430.0 2011-12-21      27.0    NaN      SPH            MAIN   \n",
       "4  300730505.0 2011-12-21  117547.0    NaN      SF1        DOMMETER   \n",
       "\n",
       "  meter_install_date meter_model network  postcode          suburb  \\\n",
       "0                NaN         NaN      SM    6110.0  SOUTHERN RIVER   \n",
       "1                NaN         NaN      SM    6111.0       KELMSCOTT   \n",
       "2                NaN         NaN      NM    6019.0   WEMBLEY DOWNS   \n",
       "3                NaN         NaN      NM    6026.0        WOODVALE   \n",
       "4         14.01.2014         M8A      SM    6153.0             NaN   \n",
       "\n",
       "   supply_pressure   station_name  distance  \n",
       "0              NaN  JANDAKOT AERO  0.131159  \n",
       "1              NaN        BICKLEY  0.099496  \n",
       "2              NaN     SWANBOURNE  0.049014  \n",
       "3              NaN    PERTH METRO  0.138307  \n",
       "4             1.25  JANDAKOT AERO  0.087419  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add station name to maintenance data\n",
    "df_maint = df_maint.merge(clean_postcodes_stn, on='postcode', how='left')\n",
    "\n",
    "df_maint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = path.join(data_folder, 'HSdf_maint(afterpostcode).csv')\n",
    "df_maint.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 570456 entries, 0 to 570455\n",
      "Data columns (total 14 columns):\n",
      "notif                 570456 non-null float64\n",
      "notif_date            570456 non-null datetime64[ns]\n",
      "floc                  570394 non-null float64\n",
      "order                 261349 non-null float64\n",
      "job_type              570402 non-null category\n",
      "catalog_profile       554056 non-null object\n",
      "meter_install_date    462138 non-null object\n",
      "meter_model           461880 non-null object\n",
      "network               554074 non-null category\n",
      "postcode              554070 non-null float64\n",
      "suburb                39478 non-null object\n",
      "supply_pressure       474695 non-null float64\n",
      "station_name          554066 non-null object\n",
      "distance              554066 non-null float64\n",
      "dtypes: category(2), datetime64[ns](1), float64(6), object(5)\n",
      "memory usage: 57.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_maint.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_weather.cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weather data to maintenance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weather to maintenance data\n",
    "# Use weather station name and date as keys\n",
    "df_complete = pd.merge(df_maint, clean_weather, left_on=['station_name','notif_date'], \n",
    "                       right_on=['station_name','weather_date'], how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsmith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# New functions to deal with defaulting the weather station to something other than the closest station\n",
    "# Hugh Smith\n",
    "# 25 January 2019\n",
    "\n",
    "# Where the weather data can't be found - first lets see if we can default the PERTH METRO weather \n",
    "df_weather_incomplete = df_complete[df_complete['max_temp'].isnull()]\n",
    "df_weather_incomplete.loc[:, ('station_name')] = 'PERTH METRO'\n",
    "\n",
    "# Nows lets remerge this new set with the weather data\n",
    "df_weather_incomplete=df_weather_incomplete.drop(['evapo_trans_0000_2400', 'rain_0900_0900', 'pan_evap_0900_0900', 'max_temp', 'min_temp', 'max_rel_humidity', 'min_rel_humidity', 'avg_10m_wind_speed', 'solar_radiation'], axis=1)\n",
    "df_complete2 = pd.merge(df_weather_incomplete, clean_weather, left_on=['station_name','notif_date'], \n",
    "                       right_on=['station_name','weather_date'], how='left')\n",
    "\n",
    "# Now lets append the updated data back into the df_complete data\n",
    "df_complete.dropna(subset=['max_temp'],inplace=True)\n",
    "df_complete = df_complete.append(df_complete2)\n",
    "\n",
    "\n",
    "# Where the weather data still can't be found - lets see if we can default the PERTH AIRPORT weather \n",
    "df_weather_incomplete = df_complete[df_complete['max_temp'].isnull()]\n",
    "df_weather_incomplete.loc[:, ('station_name')] = 'PERTH AIRPORT'\n",
    "\n",
    "# Nows lets remerge this new set with the weather data\n",
    "df_weather_incomplete=df_weather_incomplete.drop(['evapo_trans_0000_2400', 'rain_0900_0900', 'pan_evap_0900_0900', 'max_temp', 'min_temp', 'max_rel_humidity', 'min_rel_humidity', 'avg_10m_wind_speed', 'solar_radiation'], axis=1)\n",
    "df_complete2 = pd.merge(df_weather_incomplete, clean_weather, left_on=['station_name','notif_date'], \n",
    "                       right_on=['station_name','weather_date'], how='left')\n",
    "\n",
    "# Now lets append the updated data back into the df_complete data\n",
    "df_complete.dropna(subset=['max_temp'],inplace=True)\n",
    "df_complete = df_complete.append(df_complete2)\n",
    "\n",
    "# If there continues to be incomplete weather data lets just drop those rows\n",
    "df_complete.dropna(subset=['max_temp'],inplace=True)\n",
    "\n",
    "#df_weather_incomplete['station_name'].head()\n",
    "#df_weather_incomplete.describe()\n",
    "#df_weather_incomplete.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notif_date</th>\n",
       "      <th>floc</th>\n",
       "      <th>order</th>\n",
       "      <th>job_type</th>\n",
       "      <th>catalog_profile</th>\n",
       "      <th>meter_install_date</th>\n",
       "      <th>meter_model</th>\n",
       "      <th>network</th>\n",
       "      <th>postcode</th>\n",
       "      <th>suburb</th>\n",
       "      <th>...</th>\n",
       "      <th>distance</th>\n",
       "      <th>evapo_trans_0000_2400</th>\n",
       "      <th>rain_0900_0900</th>\n",
       "      <th>pan_evap_0900_0900</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>max_rel_humidity</th>\n",
       "      <th>min_rel_humidity</th>\n",
       "      <th>avg_10m_wind_speed</th>\n",
       "      <th>solar_radiation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notif</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300713965.0</th>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>504107.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SM</td>\n",
       "      <td>6110.0</td>\n",
       "      <td>SOUTHERN RIVER</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131159</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>28.9</td>\n",
       "      <td>13.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.87</td>\n",
       "      <td>34.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300719006.0</th>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>673096.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPL</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SM</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>KELMSCOTT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099496</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>16.4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.24</td>\n",
       "      <td>34.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300719010.0</th>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>810841.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NM</td>\n",
       "      <td>6019.0</td>\n",
       "      <td>WEMBLEY DOWNS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049014</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>25.8</td>\n",
       "      <td>16.9</td>\n",
       "      <td>91.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>4.63</td>\n",
       "      <td>34.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300719430.0</th>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPH</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NM</td>\n",
       "      <td>6026.0</td>\n",
       "      <td>WOODVALE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138307</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>29.2</td>\n",
       "      <td>17.4</td>\n",
       "      <td>89.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.37</td>\n",
       "      <td>34.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300730505.0</th>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>117547.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SF1</td>\n",
       "      <td>DOMMETER</td>\n",
       "      <td>14.01.2014</td>\n",
       "      <td>M8A</td>\n",
       "      <td>SM</td>\n",
       "      <td>6153.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087419</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>28.9</td>\n",
       "      <td>13.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.87</td>\n",
       "      <td>34.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            notif_date      floc  order job_type catalog_profile  \\\n",
       "notif                                                              \n",
       "300713965.0 2011-12-21  504107.0    NaN      SPH            MAIN   \n",
       "300719006.0 2011-12-21  673096.0    NaN      SPL            MAIN   \n",
       "300719010.0 2011-12-21  810841.0    NaN      SPH            MAIN   \n",
       "300719430.0 2011-12-21      27.0    NaN      SPH            MAIN   \n",
       "300730505.0 2011-12-21  117547.0    NaN      SF1        DOMMETER   \n",
       "\n",
       "            meter_install_date meter_model network  postcode          suburb  \\\n",
       "notif                                                                          \n",
       "300713965.0                NaN         NaN      SM    6110.0  SOUTHERN RIVER   \n",
       "300719006.0                NaN         NaN      SM    6111.0       KELMSCOTT   \n",
       "300719010.0                NaN         NaN      NM    6019.0   WEMBLEY DOWNS   \n",
       "300719430.0                NaN         NaN      NM    6026.0        WOODVALE   \n",
       "300730505.0         14.01.2014         M8A      SM    6153.0             NaN   \n",
       "\n",
       "                  ...         distance evapo_trans_0000_2400  rain_0900_0900  \\\n",
       "notif             ...                                                          \n",
       "300713965.0       ...         0.131159                   7.3             0.0   \n",
       "300719006.0       ...         0.099496                   7.9             0.0   \n",
       "300719010.0       ...         0.049014                   6.4             0.0   \n",
       "300719430.0       ...         0.138307                   7.2             0.0   \n",
       "300730505.0       ...         0.087419                   7.3             0.0   \n",
       "\n",
       "             pan_evap_0900_0900  max_temp  min_temp  max_rel_humidity  \\\n",
       "notif                                                                   \n",
       "300713965.0                10.8      28.9      13.6              97.0   \n",
       "300719006.0                 8.0      30.6      16.4              82.0   \n",
       "300719010.0                36.8      25.8      16.9              91.0   \n",
       "300719430.0                 2.4      29.2      17.4              89.0   \n",
       "300730505.0                10.8      28.9      13.6              97.0   \n",
       "\n",
       "             min_rel_humidity  avg_10m_wind_speed  solar_radiation  \n",
       "notif                                                               \n",
       "300713965.0              32.0                3.87            34.47  \n",
       "300719006.0              35.0                4.24            34.47  \n",
       "300719010.0              61.0                4.63            34.24  \n",
       "300719430.0              38.0                3.37            34.38  \n",
       "300730505.0              32.0                3.87            34.47  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete = df_complete.set_index(df_complete.columns[0])\n",
    "df_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = path.join(data_folder, 'HSdf_complete.csv')\n",
    "df_complete.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='write'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write cleaned and combined data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = path.join(data_folder, 'corrective_maint_against_weather.csv')\n",
    "csv_file = path.join(data_folder, 'corrective_maint_job_types.csv')\n",
    "df_complete.to_csv(csv_file)\n",
    "\n",
    "#csv_file = path.join(data_folder, 'binary_weather.csv')\n",
    "#df_wide.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert catagorical columns to binary to enable PCA dimensionality reduction later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'weather_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3062\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3063\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'weather_date'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-27dad3a0df50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmake_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdf_wide\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_complete\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morig\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_complete\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'catalog_profile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2683\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2685\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2690\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2692\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2486\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3063\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3065\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'weather_date'"
     ]
    }
   ],
   "source": [
    "df_wide = pd.DataFrame()\n",
    "\n",
    "# Fields to directly copy to new analysis dataframe\n",
    "make_copy = (\n",
    "    ('weather_date', 'date'),\n",
    "    #('evapo_trans_0000_2400', 'evapo_trans_0000_2400'),\n",
    "    #('rain_0900_0900', 'rain_0900_0900'),\n",
    "    #('pan_evap_0900_0900', 'pan_evap_0900_0900'),\n",
    "    ('max_temp', 'max_temp'),\n",
    "    ('min_temp', 'min_temp'),\n",
    "    #('max_rel_humidity', 'max_rel_humidity'),\n",
    "    #('min_rel_humidity', 'min_rel_humidity'),\n",
    "    #('avg_10m_wind_speed', 'avg_10m_wind_speed'),\n",
    "    #('solar_radiation', 'solar_radiation')\n",
    ")\n",
    "for orig, new in make_copy:\n",
    "    df_wide[new] = df_complete[orig]\n",
    "\n",
    "cat = pd.get_dummies(df_complete['catalog_profile'])\n",
    "job = pd.get_dummies(df_complete['job_type'])\n",
    "#net = pd.get_dummies(df_complete['network'])\n",
    "#obj = pd.get_dummies(df_complete['object_part_code'])\n",
    "#dam = pd.get_dummies(df_complete['damage_code'])\n",
    "#cau = pd.get_dummies(df_complete['cause_code'])\n",
    "\n",
    "\n",
    "df_wide = df_wide.join(cat)\n",
    "df_wide = df_wide.join(job)\n",
    "#df_wide = df_wide.join(net)\n",
    "#df_wide = df_wide.append(obj, sort=False)\n",
    "#df_wide = df_wide.append(dam, sort=False)\n",
    "#df_wide = df_wide.append(cau, sort=False)\n",
    "\n",
    "\n",
    "df_wide=df_wide.set_index('date')\n",
    "df_wide=df_wide.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Analysis Frame to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = path.join(data_folder, 'binary_weather.csv')\n",
    "df_wide.to_csv(csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
